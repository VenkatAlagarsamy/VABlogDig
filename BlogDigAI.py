#Import Lagnchain packages
from langchain.chat_models import ChatOpenAI    # ChatOpenAI to communicate with OpenAI GPT-3 model
from langchain.chains import ConversationChain  # Cental component for managing the flow of conversations and maintaining conversational context
from langchain.chains.conversation.memory import ConversationBufferWindowMemory #To maintain context and improve the coherence of dialogues
from langchain.prompts import SystemMessagePromptTemplate   #To define the format and content of system messages generated by the chatbot in response to user prompts
from langchain.prompts import HumanMessagePromptTemplate    #To define the format and content of human messages (prompts)    
from langchain.prompts import ChatPromptTemplate            #To orchestrate the flow of conversations and to provide interactions between the user and AI model
from langchain.prompts import MessagesPlaceholder           #Marker/laceholder to enable the dynamic conversation

import streamlit as st  #To create and share custom web apps using Python

#import custom (BlogDig) specific packages
from global_vars import *
from BlogDigAIUtils import *     

#set API keys, urls and environments
openai_api_key = get_openai_api_key()

#Header container to show the header
header_container = st.container()
with header_container :
    st.header('Dig Venkat Alagarsamy Blog', divider='rainbow')

#Define prompt display style and div
user_html_string = '''
<style>
.bordered-container {
    border-radius: 5px;
    padding: 10px;
    background-color: #b9c5df
}
</style>
<div class="bordered-container">
    ___USER_QUERY___
</div>
'''

#Define Bot (jarvis) display styles and div
jarvis_html_string = '''
<style>
    .bordered-container-jarvis {
    border-radius: 5px;
    padding: 10px;
    background-color: #d8dadc
}
</style>
<div class="bordered-container-jarvis">
    ___JARVIS_RESPONSE___
</div>
'''

#intizialize chat environments with seesion_state and buffer_memory
if 'responses' not in st.session_state:
    st.session_state['responses'] = ["How can I assist you?"]

if 'requests' not in st.session_state:
    st.session_state['requests'] = []

if 'buffer_memory' not in st.session_state :
    st.session_state.buffer_memory=ConversationBufferWindowMemory(k=3, return_messages=True)


#Define Template to display format and content of system messages
system_msg_template = SystemMessagePromptTemplate.from_template(template="""Answer the question as truthfully as possible using the provided context, and if the answer is not contained with the text below, say 'I don't know""" )

#Define Template to display format and content of user inputs (queries)
human_msg_template = HumanMessagePromptTemplate.from_template(template='{input}')

#Define Template to get the chat prompt from user
prompt_template = ChatPromptTemplate.from_messages([system_msg_template, MessagesPlaceholder(variable_name='history'), human_msg_template])

#Define llm model 
llm = ChatOpenAI(model_name="gpt-3.5-turbo", openai_api_key=openai_api_key)
#Define chain to maintain conversation context
conversation = ConversationChain(memory=st.session_state.buffer_memory, prompt=prompt_template, llm=llm, verbose=True)

#define containers to show the response and user input prompt
response_container = st.container() #Chat History (response) Container
request_container = st.container()     #User Input (request) container

#get user prompt
prompt = st.chat_input('Prompt: ', max_chars=1000)

with request_container :
    
    #if user given a query input
    if prompt:
        with st.spinner('Working...') : #Show working Spinner

            conversation_string = get_conversation_string()  #To get the entire conversation from session
            #print(f'conversation_string: {conversation_string}') 

            refined_query = get_refined_query(conversation_string, prompt) #Refine query using the conversation in session and ai model
            #print(f'refined_query: {refined_query}')

            context = get_chat_context(refined_query) #from the conversation get(find) the nearest matched query and return chat context
            #print (f'context: {context}')

            predict_input = f'Context:\n {context} \n\n Query:\n {prompt}' #Concatenate context retured with user new prompt to set the optimum query(prompt) string
            #print (f'Predict Input: {predict_input}')
            response = conversation.predict(input=predict_input) #this langchain method generate reponses in a conversational AI. I/P: prompt & conversation History O/P: Predicted Response

        #Append seesion state with new user prompt and predicted response
        st.session_state.requests.append(prompt)
        st.session_state.responses.append(response) 

with response_container :
    if st.session_state['responses']:

        for i in range(len(st.session_state['responses'])):

            with st.columns([0.9,0.1])[0]: #split response div with 2 columns as the col1[0] as 90% and Col2[1] as 10% 
                with st.chat_message('assistant') :  #ai icon
                    jarvis_html = jarvis_html_string.replace('___JARVIS_RESPONSE___', st.session_state['responses'][i]) #Replace jarvis html place holder (___JARVIS_RESPONSE___) with predicted response using session state
                    col1, col2 = st.columns([0.9,0.1]) #split col1 (90%) to another two colums. col1 to heep the reponse text and col2 toplace the Speak button
                    with col1:
                        st.markdown(jarvis_html, unsafe_allow_html=True) #Show AI response
                    with col2:
                        st.button('S', on_click=text_2_voice, args=[i], key=f'button_{i}') #Show Speak Button

            with st.columns([0.2,0.8])[1]: #split request div with 2 columns so that the columns 2 could be right aligned 
                with st.chat_message('user') : #user icon
                    if i < len(st.session_state['requests'] ):
                        user_html = user_html_string.replace('___USER_QUERY___', st.session_state['requests'][i]) #replace user_html with user prompt(s)
                        st.markdown(user_html, unsafe_allow_html=True) #Show user prompts

       
                    
                        

                
